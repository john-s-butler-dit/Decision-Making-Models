{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Audio Visual Reaction Time Simulation\n",
    "This notebook use the decision making model by Wang and colleagues to simulate linear audio-visual reaction time task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.12\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Library Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARY\n",
    "## DDM LIBRARY\n",
    "import ddm.plot\n",
    "from ddm import Model, Fittable\n",
    "from ddm.models import DriftConstant, NoiseConstant, BoundConstant, OverlayChain,OverlayNonDecision,OverlayPoissonMixture,BoundCollapsingExponential\n",
    "from ddm.functions import fit_adjust_model, display_model\n",
    "from ddm import Sample\n",
    "from ddm.plot import model_gui\n",
    "from ddm.models import LossRobustBIC\n",
    "from ddm.functions import fit_adjust_model, display_model\n",
    "\n",
    "\n",
    "## LIBRARIES\n",
    "import numpy as np # vector manipulation\n",
    "import math  # math functions\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "# THIS IS FOR PLOTTING\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# get ANOVA table as R like output\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import ptitprince as pt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "import warnings\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rcParams['text.color'] = \"black\"\n",
    "plt.rcParams['axes.labelcolor'] = \"black\"\n",
    "plt.rcParams['xtick.color'] = \"black\"\n",
    "plt.rcParams['ytick.color'] = \"black\"\n",
    "#, 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'})\n",
    "\n",
    "plt.rc('font', **font)\n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reduced Network Model\n",
    "\n",
    "The firing rate function, the input-output, \n",
    "$$ H(x)=\\frac{ax-b}{1-e^{-d(ax-b)}},$$\n",
    "a=270, b=108 and d=0.154."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(x):\n",
    "    a=270 # Hz/nA\n",
    "    b=108 # Hz\n",
    "    d=.154 # seconds\n",
    "    f=(a*x-b)/(1-np.exp(-d*(a*x-b)))\n",
    "    return f\n",
    "x=np.arange(-1,1,0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Circuit\n",
    "\n",
    "### Unisensory version\n",
    "$$ x_{1}=J_{11}S_1-J_{12}S_2+I_{0}+I_{1}+I_{noise,1}$$\n",
    "$$ x_{2}=J_{22}S_2-J_{21}S_1+I_{0}+I_{2}+I_{noise,1}$$\n",
    "\n",
    "where the synaptic couplings are $J_{11}=0.2609$, $J_{22}=0.2609$, $J_{12}=0.0497$ and $J_{21}=0.0497$.\n",
    "$I_{0}=0.3255 nA$ represents external input and $S_1$ and $S_2$ are the auditory activity $A_1$ and $A_2$ or visual activity $V_1$ and $V_2$.\n",
    "Where 1 is HIT and 2 in MISS\n",
    "\n",
    "\n",
    "### Multisensory version\n",
    "TO BE TESTED?\n",
    "$$ x_{HIT}=J_{11}(A_1+V_1)-J_{12}(A_2+V_2)+I_{0}+I_{1}+I_{noise,1}$$\n",
    "$$ x_{MISS}=J_{22}(A_2+V_2)-J_{21}(A_1+V_1)+I_{0}+I_{2}+I_{noise,1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_synaptic_current(S_1,S_2,I_1,I_2,I_noise_1,I_noise_2):\n",
    "    # Synaptic coupling\n",
    "    J_11=0.2609 # nA\n",
    "    J_22=0.2609 # nA\n",
    "    J_13=0*0.000497/np.sqrt(2) # nA\n",
    "    J_24=0*0.000497/np.sqrt(2) # nA\n",
    "  \n",
    "    J_12=0.0497 # nA\n",
    "    J_21=0.0497 # nA\n",
    "    I_0=0.3255  # nA\n",
    "    x_1=J_11*S_1-J_12*S_2+I_0+I_1+I_noise_1\n",
    "    x_2=J_22*S_2-J_21*S_1+I_0+I_2+I_noise_2\n",
    "    return x_1, x_2\n",
    "\n",
    "\n",
    "def MULTISENSORY_total_synaptic_current(A_1,A_2,V_1,V_2,I_1,I_2,I_noise_1,I_noise_2):\n",
    "    # Synaptic coupling\n",
    "    J_11=0.2609 # nA\n",
    "    J_22=0.2609 # nA\n",
    "    J_13=0*0.000497/np.sqrt(2) # nA\n",
    "    J_24=0*0.000497/np.sqrt(2) # nA\n",
    "  \n",
    "    J_12=0.0497 # nA\n",
    "    J_21=0.0497 # nA\n",
    "    I_0=0.3255  # nA\n",
    "    x_1=J_11*(A_1+V_1)-J_12*(A_2+V_2)+I_0+I_1+I_noise_1\n",
    "    x_2=J_22*(A_2+V_2)-J_21*(A_1+V_1)+I_0+I_2+I_noise_2\n",
    "    return x_1, x_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background activity\n",
    "$$ \\tau_{AMPA}\\frac{d I_{noise,i}(t)}{dt} =-I_{noise,i}(t)+\\eta_i(t)\\sqrt{\\tau_{AMPA}}\\sigma_{noise}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Background_Activity(I_noise):\n",
    "    h=0.1\n",
    "    sigma_noise=0.02 # nA\n",
    "    tau_AMPA=2 #ms\n",
    "    eta_noise=np.random.normal(0,1,1)\n",
    "    k=0#(-(I_noise)+eta_noise*np.sqrt(tau_AMPA)*sigma_noise)\n",
    "    I_noise_new=I_noise+h/tau_AMPA*(-(I_noise+h/2*k)+eta_noise\n",
    "                                *np.sqrt(tau_AMPA)*sigma_noise)\n",
    "    return I_noise_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Dynamics\n",
    "$$ \\frac{d S_{i}}{dt} =-\\frac{S_{i}}{\\tau_S}+(1-S_{i})\\gamma H_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network_Dynamics_VIS(S,x,tau_S=0.1):\n",
    "    h=0.1/1000 #ms\n",
    "    gamma=0.641\n",
    "    k=(-S/tau_S+(1-S)*gamma*H(x)/1)\n",
    "    S_new=S+h*(-(S+h/2*k)/tau_S+(1-S+h/2*k)*gamma*H(x))\n",
    "    return S_new\n",
    "\n",
    "def Network_Dynamics_AUDIO(S,x,tau_S=0.1):\n",
    "    h=0.1/1000 #ms\n",
    "    gamma=0.641\n",
    "    #tau_S=.10 #s\n",
    "    k=(-S/tau_S+(1-S)*gamma*H(x)/1)\n",
    "    S_new=S+h*(-(S+h/2*k)/tau_S+(1-S+h/2*k)*gamma*H(x))\n",
    "    return S_new\n",
    "\n",
    "def Network_Dynamics_AV(S,x,tau_S=0.1):\n",
    "    h=0.1/1000 #ms\n",
    "    gamma=0.641\n",
    "    #tau_S=.10 #s\n",
    "    k=(-S/tau_S+(1-S)*gamma*H(x)/1)\n",
    "    S_new=S+h*(-(S+h/2*k)/tau_S+(1-S+h/2*k)*gamma*H(x))\n",
    "    return S_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Current Target\n",
    "$c'$ is coherence in this formula but we use $c'$ as strenght of stimulus\n",
    "$$ I_i=J_{A,ext}\\mu_0\\left(1+ \\frac{c'}{100} \\right) $$\n",
    "default at 10 is close to 100 % hits but not quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def I_input_1(c_dash):\n",
    "    J_A_ext=5.2/10000 # nA/Hz\n",
    "    mu_0=30 # Hz\n",
    "    I_motion=J_A_ext*mu_0*(1+(c_dash)/100)\n",
    "    return I_motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I_2=J_{A,ext}\\mu_0\\left(1- \\frac{c'}{100} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def I_input_2(c_dash):\n",
    "    J_A_ext=0.00052 # nA/Hz\n",
    "    mu_0=30 # Hz\n",
    "    I_motion=J_A_ext*mu_0*(1-(c_dash)/100)\n",
    "    return I_motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaction Time Function\n",
    "This function detects when firing rate goes above Threshold.\n",
    "It takes in Firing rate for HIT and MISS, threshold and time, it returns ANSWER, Reation Time (RT) and count.\n",
    "If count =1 there has been a response if count =0 no response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reaction_Time_UNI(Firing_Rate_1,Firing_Rate_2,Threshold,time):\n",
    "    ANSWER=0\n",
    "    RT=0\n",
    "    count=0\n",
    "    if (Firing_Rate_1>=Threshold ): \n",
    "        ANSWER=1\n",
    "        RT=time\n",
    "        count=1\n",
    "    elif (Firing_Rate_2>=Threshold):\n",
    "        ANSWER=0\n",
    "        RT=time\n",
    "        count=1\n",
    "    return ANSWER,RT,count     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multisensory Winner Take All\n",
    "The function takes in both Audio and Visual activity and checks which pass threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reaction_Time_MULT(FR_Audio_HIT,Firing_Rate_2,FR_Video_HIT,Firing_Rate_4,Threshold,time):\n",
    "    ANSWER=0\n",
    "    RT=0\n",
    "    count=0\n",
    "    if (FR_Audio_HIT>=Threshold )| (FR_Video_HIT >=Threshold): \n",
    "        ANSWER=1\n",
    "        RT=time\n",
    "        count=1\n",
    "    elif (Firing_Rate_2>=Threshold)|(Firing_Rate_4 >=Threshold):\n",
    "        ANSWER=0\n",
    "        #RT=time\n",
    "        count=1\n",
    "    return ANSWER,RT,count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up time\n",
    "Each epoch (trial) is between -100 ms and 1500ms.\n",
    "The Threshold is set to 20 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.1\n",
    "time=np.arange(-100,1500,h)\n",
    "J_A_ext=0.00052 # nA/Hz\n",
    "mu_0=30 # Hz\n",
    "STIMULUS=[12.0]#,7.5,10.0,15.0]\n",
    "Threshold=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "* K is number of \"participants\"\n",
    "* N is number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=50 #51\n",
    "N=501\n",
    "#TAU_AUDIO=np.random.uniform(low=0.085, high=0.12, size=K)\n",
    "#TAU_VIDEO=np.random.uniform(low=0.085, high=0.12, size=K)\n",
    "minV, maxV= 0.085,0.115\n",
    "# create one-dimensional arrays for x and y\n",
    "x = np.arange(minV, maxV+0.001, (maxV-minV)/7.)\n",
    "y = np.arange(minV, maxV+0.001, (maxV-minV)/7.)\n",
    "# create the mesh based on these arrays\n",
    "TAU_AUDIO, TAU_VIDEO = np.meshgrid(x, y)\n",
    "\n",
    "df_TAU=pd.DataFrame({'Audio': TAU_AUDIO.flatten(), 'Visual': TAU_VIDEO.flatten()})\n",
    "\n",
    "TAU_AUDIO=TAU_AUDIO.flatten()\n",
    "TAU_VIDEO=TAU_VIDEO.flatten()\n",
    "K=len(TAU_AUDIO.flatten())\n",
    "\n",
    "#df_TAU.to_csv('DATA/TAU.csv')\n",
    "TAU_AUDIO\n",
    "#TAU_VIDEO\n",
    "RT_AUDIO_coh_hit=[]\n",
    "RT_AUDIO_coh_miss=[]#np.zeros(len(Vector_coherence))\n",
    "Prob_AUDIO=[]#np.zeros(len(Vector_coherence))\n",
    "RT_VIS_coh_hit=[]#np.zeros(len(Vector_coherence))\n",
    "RT_VIS_coh_miss=[]#np.zeros(len(Vector_coherence))\n",
    "Prob_VIS=[]#np.zeros(len(Vector_coherence))\n",
    "\n",
    "\n",
    "GROUP_RT=np.zeros((3,K))\n",
    "GROUP_ACC=np.zeros((3,K))\n",
    "PRIMED_RT=np.zeros((3,K))\n",
    "PRIMED_ACC=np.zeros((3,K))\n",
    "NOT_PRIMED_RT=np.zeros((3,K))\n",
    "NOT_PRIMED_ACC=np.zeros((3,K))\n",
    "PRIMED_Drift=np.zeros((3,K))\n",
    "NOT_PRIMED_Drift=np.zeros((3,K))\n",
    "PRIMED_time_delay=np.zeros((3,K))\n",
    "NOT_PRIMED_time_delay=np.zeros((3,K))\n",
    "\n",
    "ALL_F_1=0.2*np.ones((N,len(time)))\n",
    "ALL_F_2=0.2*np.ones((N,len(time)))\n",
    "I_VIS_HIT=0.0*np.ones(len(time)) \n",
    "I_VIS_MISS=0.0*np.ones(len(time)) \n",
    "I_AUDIO_HIT=0.0*np.ones(len(time)) \n",
    "I_AUDIO_MISS=0.0*np.ones(len(time)) \n",
    "\n",
    "Firing_target_VIS_HIT=0*time # np.zeros((1,len(time)))\n",
    "Firing_target_VIS_MISS=0*time # np.zeros((1,len(time)))\n",
    "Firing_target_AUDIO_HIT=0*time # np.zeros((1,len(time)))\n",
    "Firing_target_AUDIO_MISS=0*time # np.zeros((1,len(time)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_TYPE=np.random.randint(3, size=N) \n",
    "TRIAL_REPEAT=np.random.randint(2, size=N) \n",
    "AUDIO_REPEAT=np.random.randint(2, size=N) \n",
    "VISUAL_REPEAT=np.random.randint(2, size=N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_REPEAT=np.zeros(N) \n",
    "VISUAL_REPEAT=np.zeros(N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_TYPE=np.random.randint(3, size=N) \n",
    "AUDIO_REPEAT[((TRIAL_TYPE==1) | (TRIAL_TYPE==2))]=1\n",
    "VISUAL_REPEAT[((TRIAL_TYPE==0) | (TRIAL_TYPE==2))]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_Drift=np.zeros(K)\n",
    "V_Drift=np.zeros(K)\n",
    "A_Drift=np.zeros(K)\n",
    "Pred_Drift=np.zeros(K)\n",
    "\n",
    "\n",
    "AV_time_delay=np.zeros(K)\n",
    "V_time_delay=np.zeros(K)\n",
    "A_time_delay=np.zeros(K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant Variability\n",
    "Here each participant gets their own audio time constant $\\tau_A$ which is chosen randomly from a flat distribution from $(0.007, 0.013)$.\n",
    "For simplicities sake $\\tau_V=0.009$ and held constant for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.085, 0.09 , 0.095, 0.1  , 0.105, 0.11 , 0.115, 0.12 , 0.085,\n",
       "       0.09 , 0.095, 0.1  , 0.105, 0.11 , 0.115, 0.12 , 0.085, 0.09 ,\n",
       "       0.095, 0.1  , 0.105, 0.11 , 0.115, 0.12 , 0.085, 0.09 , 0.095,\n",
       "       0.1  , 0.105, 0.11 , 0.115, 0.12 , 0.085, 0.09 , 0.095, 0.1  ,\n",
       "       0.105, 0.11 , 0.115, 0.12 , 0.085, 0.09 , 0.095, 0.1  , 0.105,\n",
       "       0.11 , 0.115, 0.12 , 0.085, 0.09 , 0.095, 0.1  , 0.105, 0.11 ,\n",
       "       0.115, 0.12 , 0.085, 0.09 , 0.095, 0.1  , 0.105, 0.11 , 0.115,\n",
       "       0.12 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TAU_AUDIO=np.random.uniform(low=0.085, high=0.12, size=K)\n",
    "#TAU_VIDEO=np.random.uniform(low=0.085, high=0.12, size=K)\n",
    "minV, maxV= 0.085,0.12\n",
    "# create one-dimensional arrays for x and y\n",
    "x = np.arange(minV, maxV+0.001, (maxV-minV)/7.)\n",
    "y = np.arange(minV, maxV+0.001, (maxV-minV)/7.)\n",
    "# create the mesh based on these arrays\n",
    "TAU_AUDIO, TAU_VIDEO = np.meshgrid(x, y)\n",
    "\n",
    "df_TAU=pd.DataFrame({'Audio': TAU_AUDIO.flatten(), 'Visual': TAU_VIDEO.flatten()})\n",
    "\n",
    "TAU_AUDIO=TAU_AUDIO.flatten()\n",
    "TAU_VIDEO=TAU_VIDEO.flatten()\n",
    "K=len(TAU_AUDIO.flatten())\n",
    "\n",
    "#df_TAU.to_csv('DATA/TAU.csv')\n",
    "TAU_AUDIO\n",
    "#TAU_VIDEO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DDM function\n",
    "\"Participants\" Audio, Visual and Audio-Visual Reaction time and accuracy data are each submitted inturn to the general drift diffusion model to fit a drift rate (k) and time delay ($\\tau_{r}$).\n",
    "* Drift range is between 5 and 14.\n",
    "* Noise is set to 1.5 (standard)\n",
    "* Bound is set to 2.5\n",
    "For a simple reaction time the drift rate for combination of sense is predicted by \n",
    "$$ \\hat{k}_{AV}=\\sqrt{k_A^2+k_V^2},$$\n",
    "where $k_A$ and $k_V$ are the audio and visual drift rates and $\\hat{k}_{AV}$ is the predicted drift rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDM_FIT(RT,ANSWER):\n",
    "    df=[]\n",
    "    \n",
    "    # RT is scalles to seconds, the function takes seconds\n",
    "    df=pd.DataFrame({'RT': RT/1000, 'correct': ANSWER})\n",
    "    df.head()\n",
    "    sample = Sample.from_pandas_dataframe(df, rt_column_name=\"RT\",\n",
    "                                                  correct_column_name=\"correct\")\n",
    "    model = Model(name='Model',\n",
    "                      drift=DriftConstant(drift=Fittable(minval=6, maxval=25)),\n",
    "                      noise=NoiseConstant(noise=1.5),#(noise=Fittable(minval=0.5, maxval=2.5)),\n",
    "                      bound=BoundConstant(B=2.5),\n",
    "                      overlay=OverlayChain(overlays=[OverlayNonDecision(nondectime=Fittable(minval=0, maxval=.8)),\n",
    "                                                    OverlayPoissonMixture(pmixturecoef=.02,\n",
    "                                                                          rate=1)]),\n",
    "                      dx=.001, dt=.01, T_dur=2)\n",
    "\n",
    "    # Fitting this will also be fast because PyDDM can automatically\n",
    "    # determine that DriftCoherence will allow an analytical solution.\n",
    "    fit_model = fit_adjust_model(sample=sample, model=model,fitting_method=\"differential_evolution\",\n",
    "                     lossfunction=LossRobustBIC,verbose=False)\n",
    "\n",
    "    param=fit_model.get_model_parameters()\n",
    "\n",
    "    Drift=np.asarray(param[0])\n",
    "    Delay=np.asarray(param[1])\n",
    "  \n",
    "    return Drift,Delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "Three loops, Participant (k and K), Trials (n and N) and time (i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 0 of 63 n: 0 of 500\n",
      "k: 0 of 63 n: 50 of 500\n",
      "k: 0 of 63 n: 100 of 500\n",
      "k: 0 of 63 n: 150 of 500\n",
      "k: 0 of 63 n: 200 of 500\n",
      "k: 0 of 63 n: 250 of 500\n",
      "k: 0 of 63 n: 300 of 500\n",
      "k: 0 of 63 n: 350 of 500\n",
      "k: 0 of 63 n: 400 of 500\n",
      "k: 0 of 63 n: 450 of 500\n",
      "k: 0 of 63 n: 500 of 500\n",
      "Params [7.04458786 0.43848388] gave -724.2262694047665\n",
      "Params [6.73486425 0.42325631] gave -648.9305675501753\n",
      "Params [8.58004359 0.43154842] gave -990.8344603870312\n",
      "Params [7.49395739 0.43178743] gave -538.7331450083202\n",
      "Params [6.85194876 0.41417314] gave -447.7395472514164\n",
      "Params [9.6727566  0.45817036] gave -363.1884237437469\n",
      "Params [6.77734673 0.46868383] gave -205.34523700025125\n",
      "Params [6.87342408 0.46721402] gave -217.12348699780483\n",
      "k: 1 of 63 n: 0 of 500\n",
      "k: 1 of 63 n: 50 of 500\n",
      "k: 1 of 63 n: 100 of 500\n",
      "k: 1 of 63 n: 150 of 500\n",
      "k: 1 of 63 n: 200 of 500\n",
      "k: 1 of 63 n: 250 of 500\n",
      "k: 1 of 63 n: 300 of 500\n",
      "k: 1 of 63 n: 350 of 500\n",
      "k: 1 of 63 n: 400 of 500\n",
      "k: 1 of 63 n: 450 of 500\n",
      "k: 1 of 63 n: 500 of 500\n",
      "Params [7.76213803 0.26024119] gave -861.7493946640951\n",
      "Params [7.25094683 0.44594433] gave -744.5211979065247\n",
      "Params [8.20856406 0.27954103] gave -954.6498869778981\n",
      "Params [8.28387908 0.26598519] gave -620.9587248608019\n",
      "Params [7.47771621 0.43902281] gave -533.063668872366\n",
      "Params [8.28790483 0.27073538] gave -317.74418555785877\n",
      "Params [7.54228123 0.29235983] gave -270.1367790366756\n",
      "Params [7.2171074  0.48187609] gave -235.37970011020417\n",
      "k: 2 of 63 n: 0 of 500\n",
      "k: 2 of 63 n: 50 of 500\n",
      "k: 2 of 63 n: 100 of 500\n",
      "k: 2 of 63 n: 150 of 500\n",
      "k: 2 of 63 n: 200 of 500\n",
      "k: 2 of 63 n: 250 of 500\n",
      "k: 2 of 63 n: 300 of 500\n",
      "k: 2 of 63 n: 350 of 500\n",
      "k: 2 of 63 n: 400 of 500\n",
      "k: 2 of 63 n: 450 of 500\n",
      "k: 2 of 63 n: 500 of 500\n",
      "Params [8.12565282 0.17672906] gave -831.4781400713424\n",
      "Params [7.04604892 0.43674096] gave -720.5398541821775\n",
      "Params [8.15012613 0.17436267] gave -921.3889563383221\n",
      "Params [8.73753149 0.17208092] gave -655.0176135567935\n",
      "Params [7.41767933 0.43166958] gave -522.6476393501441\n",
      "Params [9.16927659 0.18764005] gave -356.17613266464156\n",
      "Params [7.29072567 0.18598653] gave -209.27691909232243\n",
      "Params [6.92180562 0.46427757] gave -211.3866339050909\n",
      "k: 3 of 63 n: 0 of 500\n",
      "k: 3 of 63 n: 50 of 500\n",
      "k: 3 of 63 n: 100 of 500\n",
      "k: 3 of 63 n: 150 of 500\n",
      "k: 3 of 63 n: 200 of 500\n",
      "k: 3 of 63 n: 250 of 500\n",
      "k: 3 of 63 n: 300 of 500\n",
      "k: 3 of 63 n: 350 of 500\n",
      "k: 3 of 63 n: 400 of 500\n",
      "k: 3 of 63 n: 450 of 500\n",
      "k: 3 of 63 n: 500 of 500\n",
      "Params [9.60526319 0.13214467] gave -965.6665322474637\n",
      "Params [6.95547874 0.44299737] gave -666.3751507997537\n",
      "Params [9.0753224  0.12290361] gave -1010.6740200044019\n",
      "Params [10.07416363  0.133588  ] gave -715.1185251505638\n",
      "Params [7.17970716 0.4394711 ] gave -485.21443888198655\n",
      "Params [9.72767166 0.12339281] gave -371.23814874217766\n",
      "Params [9.45696128 0.15253508] gave -263.7687732996323\n",
      "Params [6.91451193 0.48203412] gave -204.89861144588576\n",
      "k: 4 of 63 n: 0 of 500\n",
      "k: 4 of 63 n: 50 of 500\n",
      "k: 4 of 63 n: 100 of 500\n",
      "k: 4 of 63 n: 150 of 500\n",
      "k: 4 of 63 n: 200 of 500\n",
      "k: 4 of 63 n: 250 of 500\n",
      "k: 4 of 63 n: 300 of 500\n",
      "k: 4 of 63 n: 350 of 500\n",
      "k: 4 of 63 n: 400 of 500\n",
      "k: 4 of 63 n: 450 of 500\n",
      "k: 4 of 63 n: 500 of 500\n",
      "Params [9.60129818 0.08813144] gave -957.8772251798204\n",
      "Params [7.08937492 0.4354115 ] gave -716.4413835200319\n",
      "Params [9.53223832 0.08710398] gave -976.8422240551653\n",
      "Params [10.13695485  0.08266237] gave -747.1879754244978\n",
      "Params [7.60565612 0.43526972] gave -553.9026854211619\n",
      "Params [10.12806422  0.08764705] gave -377.5638373982581\n",
      "Params [9.68095158 0.11664206] gave -237.0066485197883\n",
      "Params [6.52834697 0.45681884] gave -188.6226750467472\n",
      "k: 5 of 63 n: 0 of 500\n",
      "k: 5 of 63 n: 50 of 500\n",
      "k: 5 of 63 n: 100 of 500\n",
      "k: 5 of 63 n: 150 of 500\n"
     ]
    }
   ],
   "source": [
    "## PARTICIPANT LOOP\n",
    "for k in range(0,K):\n",
    "  # SETTING UP INDIVIDUAL RTS and ANSWERS (HIT 1 and MISS 0)\n",
    "    ANSWER_VIS=np.zeros(N)\n",
    "    RT_VIS=np.zeros(N)\n",
    "    ANSWER_AUDIO=np.zeros(N)\n",
    "    RT_AUDIO=np.zeros(N)\n",
    "    ANSWER_AV=np.zeros(N)\n",
    "    RT_AV=np.zeros(N)\n",
    "\n",
    "\n",
    "    tau_audio=TAU_AUDIO[k] ## PARTICIPANT TAU\n",
    "    tau_video=TAU_VIDEO[k] ## PARTICIPANT TAU\n",
    "\n",
    "    \n",
    "    for n in range(0,N): # TRIAL LOOP\n",
    "        if n%50==0:\n",
    "            print('k: %d of %d n: %d of %d' %(k,K-1,n,N-1))\n",
    "        I_noise_VIS_HIT=0.001*np.random.normal(0,1,len(time))\n",
    "        I_noise_VIS_MISS=0.001*np.random.normal(0,1,len(time))\n",
    "\n",
    "        I_noise_AUDIO_HIT=0.001*np.random.normal(0,1,len(time))\n",
    "        I_noise_AUDIO_MISS=0.001*np.random.normal(0,1,len(time))\n",
    "\n",
    "\n",
    "        x_VIS_HIT=J_A_ext*mu_0*np.random.uniform(0,1,len(time))\n",
    "        x_VIS_MISS=J_A_ext*mu_0*np.random.uniform(0,1,len(time))\n",
    "        x_AUDIO_HIT=J_A_ext*mu_0*np.random.uniform(0,1,len(time))\n",
    "        x_AUDIO_MISS=J_A_ext*mu_0*np.random.uniform(0,1,len(time))\n",
    "        \n",
    "        S_VIS_HIT=0.2*np.ones(len(time))+0.01*np.random.normal(0,1,len(time))\n",
    "        S_AUDIO_HIT=0.2*np.ones(len(time))+0.01*np.random.normal(0,1,len(time))\n",
    "        S_VIS_MISS=0.2*np.ones(len(time))+0.01*np.random.normal(0,1,len(time)) \n",
    "        S_AUDIO_MISS=0.2*np.ones(len(time))+0.01*np.random.normal(0,1,len(time)) \n",
    "\n",
    "\n",
    "        # INITIAL CONDITIONS\n",
    "        if AUDIO_REPEAT[n]==1:\n",
    "            S_AUDIO_HIT=0.2*np.ones(len(time))+0.01*np.random.normal(0,1,len(time))+0.01 # PRIME\n",
    "        if VISUAL_REPEAT[n]==1:\n",
    "            S_VIS_HIT=0.2*np.ones(len(time))+0.01*np.random.normal(0,1,len(time))+0.01 # PRIME\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    " \n",
    "\n",
    "        x_VIS_HIT,x_VIS_MISS=total_synaptic_current(S_VIS_HIT,S_VIS_MISS,\n",
    "                                                                      I_VIS_HIT,I_VIS_MISS,\n",
    "                                                                      I_noise_VIS_HIT,\n",
    "                                                                      I_noise_VIS_MISS)\n",
    "        x_AUDIO_HIT,x_AUDIO_MISS=total_synaptic_current(S_AUDIO_HIT,\n",
    "                                                                          S_AUDIO_MISS,\n",
    "                                                                          I_AUDIO_HIT,\n",
    "                                                                          I_AUDIO_MISS,\n",
    "                                                                          I_noise_AUDIO_HIT,\n",
    "                                                                          I_noise_AUDIO_MISS)\n",
    "\n",
    "        count_AUDIO=0\n",
    "        count_VIS=0\n",
    "        count_AV=0\n",
    "\n",
    "        Firing_target_VIS_HIT[0]=H(x_VIS_HIT[0])\n",
    "        Firing_target_VIS_MISS[0]=H(x_VIS_MISS[0])\n",
    "        Firing_target_AUDIO_HIT[0]=H(x_VIS_HIT[0])\n",
    "        Firing_target_AUDIO_MISS[0]=H(x_VIS_MISS[0])\n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "        # TIME LOOP\n",
    "        for i in range (0,len(time)-1):\n",
    "            if time[i] >=0 and time[i]<1000:\n",
    "                c_dash=STIMULUS[0]\n",
    "            else:\n",
    "                c_dash=0.0\n",
    "\n",
    "        \n",
    "            I_noise_VIS_HIT[i+1]=Background_Activity(I_noise_VIS_HIT[i])\n",
    "            I_noise_VIS_MISS[i+1]=Background_Activity(I_noise_VIS_MISS[i])\n",
    "            I_VIS_HIT[i+1]=I_input_1(c_dash) # VISUAL HIT INPUT\n",
    "            I_VIS_MISS[i+1]=I_input_1(-c_dash) # VISUAL MISS INPUT\n",
    "         \n",
    "         \n",
    "            S_VIS_HIT[i+1]=Network_Dynamics_VIS(S_VIS_HIT[i],x_VIS_HIT[i],tau_video)\n",
    "            S_VIS_MISS[i+1]=Network_Dynamics_VIS(S_VIS_MISS[i],x_VIS_MISS[i],tau_video)\n",
    "            \n",
    "            \n",
    "            \n",
    "            x_VIS_HIT[i+1],x_VIS_MISS[i+1]=total_synaptic_current(S_VIS_HIT[i+1],S_VIS_MISS[i+1],\n",
    "                                                                      I_VIS_HIT[i+1],I_VIS_MISS[i+1],\n",
    "                                                                      I_noise_VIS_HIT[i+1],\n",
    "                                                                      I_noise_VIS_MISS[i+1])\n",
    "            \n",
    "            I_noise_AUDIO_HIT[i+1]=Background_Activity(I_noise_AUDIO_HIT[i])\n",
    "            I_noise_AUDIO_MISS[i+1]=Background_Activity(I_noise_AUDIO_MISS[i])\n",
    "            \n",
    "            I_AUDIO_HIT[i+1]=I_input_1(c_dash) # AUDITORY HIT INPUT\n",
    "            I_AUDIO_MISS[i+1]=I_input_1(-c_dash) # AUDITORY MISS INPUT\n",
    "\n",
    "            S_AUDIO_HIT[i+1]=Network_Dynamics_AUDIO(S_AUDIO_HIT[i],x_AUDIO_HIT[i],tau_audio)\n",
    "            S_AUDIO_MISS[i+1]=Network_Dynamics_AUDIO(S_AUDIO_MISS[i],x_AUDIO_MISS[i],tau_audio)\n",
    "            \n",
    "            x_AUDIO_HIT[i+1],x_AUDIO_MISS[i+1]=total_synaptic_current(S_AUDIO_HIT[i+1],\n",
    "                                                                          S_AUDIO_MISS[i+1],\n",
    "                                                                          I_AUDIO_HIT[i+1],\n",
    "                                                                          I_AUDIO_MISS[i+1],\n",
    "                                                                          I_noise_AUDIO_HIT[i+1],\n",
    "                                                                          I_noise_AUDIO_MISS[i+1])\n",
    "            \n",
    "            \n",
    "            Firing_target_AUDIO_HIT[i+1]=H(x_AUDIO_HIT[i+1])\n",
    "            Firing_target_AUDIO_MISS[i+1]=H(x_AUDIO_MISS[i+1])\n",
    "            Firing_target_VIS_HIT[i+1]=H(x_VIS_HIT[i+1])\n",
    "            Firing_target_VIS_MISS[i+1]=H(x_VIS_MISS[i+1])\n",
    "           \n",
    "            # AV RACE MODEL REACTION TIME\n",
    "            if count_AV <0.5:\n",
    "                ANSWER_AV[n],RT_AV[n],count_AV=Reaction_Time_MULT(Firing_target_VIS_HIT[i],Firing_target_VIS_MISS[i],Firing_target_AUDIO_HIT[i],Firing_target_AUDIO_MISS[i],Threshold,time[i])\n",
    "            \n",
    "            # VISUAL REACTION TIME THRESHOLD\n",
    "            if count_VIS <0.5:\n",
    "                ANSWER_VIS[n],RT_VIS[n],count_VIS=Reaction_Time_UNI(Firing_target_VIS_HIT[i],Firing_target_VIS_MISS[i],Threshold,time[i])\n",
    "            \n",
    "            # AUDITORY REACTION TIME THRESHOLD\n",
    "            if count_AUDIO <0.5:\n",
    "                ANSWER_AUDIO[n],RT_AUDIO[n],count_AUDIO=Reaction_Time_UNI(Firing_target_AUDIO_HIT[i],Firing_target_AUDIO_MISS[i],Threshold,time[i])\n",
    "    \n",
    "    \n",
    "    # GENERATES GROUP DATA BY AVERAGING PARTICIPANTS TRIALS            \n",
    "    GROUP_RT[0,k]=np.mean(RT_AUDIO[ANSWER_AUDIO==1])\n",
    "    GROUP_RT[1,k]=np.mean(RT_VIS[ANSWER_VIS==1])\n",
    "    GROUP_RT[2,k]=np.mean(RT_AV[ANSWER_AV==1])\n",
    "    GROUP_ACC[0,k]=np.mean(ANSWER_AUDIO)\n",
    "    GROUP_ACC[1,k]=np.mean(ANSWER_VIS)\n",
    "    GROUP_ACC[2,k]=np.mean(ANSWER_AV)\n",
    "    \n",
    "    \n",
    "    PRIMED_RT[0,k]=np.mean(RT_AUDIO[(ANSWER_AUDIO==1) & (AUDIO_REPEAT==1)])\n",
    "    PRIMED_RT[1,k]=np.mean(RT_VIS[(ANSWER_VIS==1) & (VISUAL_REPEAT==1)])\n",
    "    PRIMED_RT[2,k]=np.mean(RT_AV[ANSWER_AV==1])\n",
    "    PRIMED_ACC[0,k]=np.mean(ANSWER_AUDIO[TRIAL_REPEAT==1])\n",
    "    PRIMED_ACC[1,k]=np.mean(ANSWER_VIS[TRIAL_REPEAT==1])\n",
    "    PRIMED_ACC[2,k]=np.mean(ANSWER_AV[TRIAL_REPEAT==1])\n",
    "    NOT_PRIMED_RT[0,k]=np.mean(RT_AUDIO[(ANSWER_AUDIO==1) & (AUDIO_REPEAT==0)])\n",
    "    NOT_PRIMED_RT[1,k]=np.mean(RT_VIS[(ANSWER_VIS==1) & (VISUAL_REPEAT==0)])\n",
    "    NOT_PRIMED_RT[2,k]=np.mean(RT_AV[ANSWER_AV==1])\n",
    "    NOT_PRIMED_ACC[0,k]=np.mean(ANSWER_AUDIO[AUDIO_REPEAT==0])\n",
    "    NOT_PRIMED_ACC[1,k]=np.mean(ANSWER_VIS[VISUAL_REPEAT==0])\n",
    "    NOT_PRIMED_ACC[2,k]=np.mean(ANSWER_AV[VISUAL_REPEAT==0])    \n",
    "    ## FITTING THE OUTPUTS\n",
    "    A_Drift[k],A_time_delay[k]=DDM_FIT(RT_AUDIO,ANSWER_AUDIO)\n",
    "    V_Drift[k],V_time_delay[k]=DDM_FIT(RT_VIS,ANSWER_VIS)\n",
    "    AV_Drift[k],AV_time_delay[k]=DDM_FIT(RT_AV,ANSWER_AV)\n",
    "    \n",
    "    Pred_Drift[k]=np.sqrt(A_Drift[k]*A_Drift[k]+V_Drift[k]*V_Drift[k])\n",
    "    PRIMED_Drift[0,k],PRIMED_time_delay[0,k]=DDM_FIT(RT_AUDIO[AUDIO_REPEAT==1],ANSWER_AUDIO[AUDIO_REPEAT==1])\n",
    "    PRIMED_Drift[1,k],PRIMED_time_delay[1,k]=DDM_FIT(RT_VIS[VISUAL_REPEAT==1],ANSWER_VIS[VISUAL_REPEAT==1])\n",
    "    PRIMED_Drift[2,k],PRIMED_time_delay[2,k]=DDM_FIT(RT_AV[(AUDIO_REPEAT==1) & (VISUAL_REPEAT==1)],ANSWER_AV[(AUDIO_REPEAT==1) & (VISUAL_REPEAT==1)])\n",
    "\n",
    "    NOT_PRIMED_Drift[0,k],NOT_PRIMED_time_delay[0,k]=DDM_FIT(RT_AUDIO[AUDIO_REPEAT==0],ANSWER_AUDIO[AUDIO_REPEAT==0])\n",
    "    NOT_PRIMED_Drift[1,k],NOT_PRIMED_time_delay[1,k]=DDM_FIT(RT_VIS[VISUAL_REPEAT==0],ANSWER_VIS[VISUAL_REPEAT==0])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reaction Time\n",
    "### Individual Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,4))\n",
    "plt.subplot(132)\n",
    "plt.hist(RT_VIS[VISUAL_REPEAT==1],bins=20,facecolor='red', ec=\"k\", alpha=0.5)\n",
    "plt.vlines(np.mean(RT_VIS[VISUAL_REPEAT==1]),0,210,linestyles='dashed',color='red')\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,210)\n",
    "plt.xlabel('Reaction Time',fontsize=15)\n",
    "plt.title('Visual Only ',fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(RT_AUDIO[AUDIO_REPEAT==1],bins=20,facecolor='blue', ec=\"k\", alpha=0.5)\n",
    "plt.xlabel('Reaction Time',fontsize=15)\n",
    "plt.vlines(np.mean(RT_AUDIO[AUDIO_REPEAT==1]),0,210,linestyles='dashed',color='blue')\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,210)\n",
    "\n",
    "\n",
    "plt.title('Audio Only',fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(RT_AV[ANSWER_AV==1],bins=20,facecolor='grey', ec=\"k\", alpha=0.5)\n",
    "plt.vlines(np.mean(RT_AV[ANSWER_AV==1]),0,210,color='grey',linestyles='dashed',label=\"Mean Reaction Time\")\n",
    "plt.xlabel('Reaction Time',fontsize=15)\n",
    "plt.title('Audio Visual',fontsize=20)\n",
    "plt.xlim(0,1200)\n",
    "plt.ylim(0,120)\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/PRIMED/FigureRTPRIME.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigureRTPRIME.png',dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.subplot(222)\n",
    "plt.hist(RT_VIS[(ANSWER_VIS==1) & (VISUAL_REPEAT==1)],bins=20,facecolor='red', ec=\"k\", alpha=0.3)\n",
    "plt.vlines(np.mean(RT_VIS[(ANSWER_VIS==1) & (VISUAL_REPEAT==1)]),0,100,linestyles='dashed',color='red')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel('Reaction Time (ms)',fontsize=12)\n",
    "\n",
    "plt.title('Visual Reaction Time Primed',fontsize=15)\n",
    "#plt.title('Visual Only Primed',fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(RT_AUDIO[(ANSWER_AUDIO==1) & (AUDIO_REPEAT==1)],bins=20,facecolor='blue', ec=\"k\", alpha=0.3)\n",
    "plt.xlabel('Reaction Time (ms)',fontsize=12)\n",
    "plt.vlines(np.mean(RT_AUDIO[(ANSWER_AUDIO==1) & (AUDIO_REPEAT==1)]),0,100,linestyles='dashed',color='blue')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,100)\n",
    "plt.title('Audio Reaction Time Primed',fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(RT_VIS[(ANSWER_VIS==1) & (VISUAL_REPEAT==0)],bins=20,facecolor='red', ec=\"k\", alpha=0.6)\n",
    "plt.vlines(np.mean(RT_VIS[(ANSWER_VIS==1) & (VISUAL_REPEAT==0)]),0,100,linestyles='dashed',color='red')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,100)\n",
    "\n",
    "\n",
    "plt.xlabel('Reaction Time (ms)',fontsize=12)\n",
    "plt.title('Visual Reaction Time Not-Primed',fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(RT_AUDIO[(ANSWER_AUDIO==1) & (AUDIO_REPEAT==0)],bins=20,facecolor='blue', ec=\"k\", alpha=0.6)\n",
    "plt.xlabel('Reaction Time (ms)',fontsize=12)\n",
    "plt.vlines(np.mean(RT_AUDIO[(ANSWER_AUDIO==1) & (AUDIO_REPEAT==0)]),0,100,linestyles='dashed',color='blue')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,100)\n",
    "\n",
    "plt.title('Audio Reaction Time Not-Primed',fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/RT_PRIMED_vs_NOT.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/RT_PRIMED_vs_NOT.png',dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.title('Audio Only Primed',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RT=pd.DataFrame({'Audio': GROUP_RT[0,:], 'Visual': GROUP_RT[1,:],'AV':GROUP_RT[2,:]})\n",
    "\n",
    "RT_melt = pd.melt(df_RT.reset_index(), id_vars=['index'], value_vars=['Audio', 'Visual', 'AV'])\n",
    "# replace column names\n",
    "RT_melt.columns = ['index', 'condition', 'RT']\n",
    "\n",
    "\n",
    "# Ordinary Least Squares (OLS) model\n",
    "model = ols('RT ~ C(condition)', data=RT_melt).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "anova_table\n",
    "df_RT.to_csv('DATA/PRIMED/GROUP_RT.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RT=pd.DataFrame({'Audio': PRIMED_RT[0,:], 'Visual': PRIMED_RT[1,:],'AV':PRIMED_RT[2,:]})\n",
    "df_RT.to_csv('DATA/PRIMED/GROUP_RT_PRIMED.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RT=pd.DataFrame({'Audio': NOT_PRIMED_RT[0,:], 'Visual': NOT_PRIMED_RT[1,:],'AV':NOT_PRIMED_RT[2,:]})\n",
    "\n",
    "df_RT.to_csv('DATA/PRIMED/GROUP_RT_NOT_PRIMED.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACC=pd.DataFrame({'Audio': GROUP_ACC[0,:], 'Visual': GROUP_ACC[1,:],'AV':GROUP_ACC[2,:]})\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "df_ACC.to_csv('DATA/GROUP_ACC.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDM Results\n",
    "## Drift Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Drift=pd.DataFrame({'Audio': A_Drift, 'Visual': V_Drift,'AV':AV_Drift})\n",
    "#from matplotlib import pyplot as plt\n",
    "df_Drift.to_csv('DATA/GROUP_ACC.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_delay=pd.DataFrame({'Audio': A_time_delay, 'Visual': V_time_delay,'AV':AV_time_delay})\n",
    "#from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observed and Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_Drift=np.sqrt(A_Drift*A_Drift+V_Drift*V_Drift)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(Pred_Drift,AV_Drift,'ko',markeredgewidth=2,markerfacecolor='white', markersize=10)\n",
    "plt.plot([8,14],[8,14],'k:')\n",
    "plt.ylabel('Observed AV Drift Rate',fontsize=20)\n",
    "plt.xlabel('Predicted Drift Rate',fontsize=20)\n",
    "plt.xlim(8,14)\n",
    "plt.ylim(8,14)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.axis('square')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/PRIMED/DriftvsPred.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/DriftvsPred.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_melt = pd.melt(df_ACC.reset_index(), id_vars=['index'], value_vars=['Audio', 'Visual', 'AV'])\n",
    "# replace column names\n",
    "ACC_melt.columns = ['index', 'condition', 'ACC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Drift_melt = pd.melt(df_Drift.reset_index(), id_vars=['index'], value_vars=['Audio', 'Visual', 'AV'])\n",
    "# replace column names\n",
    "Drift_melt.columns = ['index', 'condition', 'Drift']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Fearon, C., Butler, J. S., Newman, L., Lynch, T., & Reilly, R. B. (2015). Audiovisual processing is abnormal in Parkinson’s disease and correlates with freezing of gait and disease duration. Journal of Parkinson's disease, 5(4), 925-936."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "pal = \"Set2\"\n",
    "ax=sns.stripplot( x = \"condition\", y = \"RT\", data = RT_melt, palette = pal,\n",
    "edgecolor = \"white\", size = 4, jitter = 1, zorder = 0)\n",
    "ax=sns.boxplot( x = \"condition\", y = \"RT\", data = RT_melt, color = \"black\",\n",
    "width = .15, zorder = 10, showcaps = True,\n",
    "boxprops = {'facecolor':'none', \"zorder\":10}, showfliers=True, whiskerprops = {'linewidth':4, \"zorder\":10},\n",
    "saturation = 1)\n",
    "ax.set_ylim(0,1000)\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/PRIMED/FigureRTGroupPRIMED.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigureRTGroupPRIMED.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "pal = \"Set2\"\n",
    "ax=sns.stripplot( x = \"Condition\", y = \"Drift\", data = Drift_melt, palette = pal,\n",
    "edgecolor = \"white\", size = 6, jitter = 1, zorder = 0)\n",
    "ax=sns.boxplot( x = \"condition\", y = \"Drift\", data = Drift_melt, color = \"black\",\n",
    "width = .15, zorder = 10, showcaps = True,\n",
    "boxprops = {'facecolor':'none', \"zorder\":10}, showfliers=True, whiskerprops = {'linewidth':4, \"zorder\":10},\n",
    "saturation = 1)\n",
    "ax.set_ylim(6,22)\n",
    "plt.savefig('FIGURES/PRIMED/FigureDriftGroupPRIMED.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigureDriftGroupPRIMED.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr, _ = pearsonr(Pred_Drift,PRIMED_Drift[2,:])\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(Pred_Drift,PRIMED_Drift[2,:],'ko',markeredgewidth=2,markerfacecolor='white', markersize=10)\n",
    "plt.plot([8,20],[8,20],'k:')\n",
    "plt.ylabel('Observed AV Drift Rate',fontsize=20)\n",
    "plt.xlabel('Predicted Drift Rate',fontsize=20)\n",
    "plt.xlim(8,14)\n",
    "plt.ylim(8,14)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.axis('square')\n",
    "#plt.title('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/PRIMED/FigurePredictedPRIMED1.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigurePredictedPRIMED1.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMED_Pred_Drift=np.sqrt(PRIMED_Drift[0,:]*PRIMED_Drift[0,:]+PRIMED_Drift[1,:]*PRIMED_Drift[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr, _ = pearsonr(PRIMED_Pred_Drift,PRIMED_Drift[2,:])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(PRIMED_Pred_Drift,PRIMED_Drift[2,:],'ko',markeredgewidth=2,markerfacecolor='white', markersize=10)\n",
    "plt.plot([8,16],[8,16],'k:')\n",
    "plt.ylabel('Observed AV Drift Rate',fontsize=20)\n",
    "plt.xlabel('Predicted Primed Drift Rate',fontsize=20)\n",
    "plt.xlim(8,16)\n",
    "plt.ylim(8,16)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.axis('square')\n",
    "#plt.title('Pearsons correlation: %.3f' % corr)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigurePredictedPRIMED.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigurePredictedPRIMED.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(AUDIO_REPEAT==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RT_PRIMED=pd.DataFrame({'Audio': PRIMED_RT[0,:], 'Visual': PRIMED_RT[1,:],'AV':PRIMED_RT[2,:]})\n",
    "df_RT_NOT_PRIMED=pd.DataFrame({'Audio': NOT_PRIMED_RT[0,:], 'Visual': NOT_PRIMED_RT[1,:],'AV':NOT_PRIMED_RT[2,:]})\n",
    "\n",
    "\n",
    "RT_PRIMED_melt = pd.melt(df_RT_PRIMED.reset_index(), id_vars=['index'], value_vars=['Audio', 'Visual', 'AV'])\n",
    "# replace column names\n",
    "RT_PRIMED_melt.columns = ['index', 'condition', 'RT']\n",
    "RT_PRIMED_melt.insert(2, \"PRIMED\", np.ones(RT_PRIMED_melt.shape[0]),True)\n",
    "RT_NOT_PRIMED_melt = pd.melt(df_RT_NOT_PRIMED.reset_index(), id_vars=['index'], value_vars=['Audio', 'Visual', 'AV'])\n",
    "# replace column names\n",
    "RT_NOT_PRIMED_melt.columns = ['index', 'condition', 'RT']\n",
    "RT_NOT_PRIMED_melt.columns = ['index', 'condition', 'RT']\n",
    "RT_NOT_PRIMED_melt.insert(2, \"PRIMED\", np.zeros(RT_PRIMED_melt.shape[0]), True)\n",
    "frames=[RT_PRIMED_melt,RT_NOT_PRIMED_melt]\n",
    "RT=pd.concat(frames)\n",
    "# Ordinary Least Squares (OLS) model\n",
    "model = ols('RT ~ C(PRIMED)+C(condition)', data=RT).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "anova_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.anova import AnovaRM\n",
    "aovrm2way = AnovaRM(RT, 'RT', 'index', within=['condition', 'PRIMED'])\n",
    "res2way = aovrm2way.fit()\n",
    "\n",
    "print(res2way)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RT_PRIMED=pd.DataFrame({'Audio': PRIMED_RT[0,:], 'Visual': PRIMED_RT[1,:],'AV':PRIMED_RT[2,:],'PRIMED':np.ones(K)})\n",
    "df_RT_NOT_PRIMED=pd.DataFrame({'Audio': NOT_PRIMED_RT[0,:], 'Visual': NOT_PRIMED_RT[1,:],'AV':NOT_PRIMED_RT[2,:],'PRIMED':np.zeros(K)})\n",
    "\n",
    "\n",
    "result = [df_RT_PRIMED,df_RT_NOT_PRIMED]\n",
    "RT_PRIMED_melt = pd.melt(df_RT_PRIMED.reset_index(), id_vars=['index'], value_vars=['Audio', 'Visual', 'AV'])\n",
    "\n",
    "result\n",
    "result = pd.concat(result)\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "result.boxplot(ax=ax, sym='',by='PRIMED',layout=(1,3),grid=False, rot=45, fontsize=15)\n",
    "plt.ylim([0,700])\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/PRIMED/FigureRT_InteractionPRIMED.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigureRT_InteractionPredictedPRIMED.png',dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "ax = sns.boxplot(x=\"condition\", y=\"RT\", data=RT,hue=\"PRIMED\",width=0.4)\n",
    "plt.ylim([0,800])\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/PRIMED/FigureRT_InteractionPRIMED.eps',dpi=300)\n",
    "\n",
    "plt.savefig('FIGURES/PRIMED/FigureRT_InteractionPredictedPRIMED.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.xkcd():\n",
    "    fig, ax= plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True,figsize=(14, 4),dpi=300)\n",
    "    camera=Camera(fig)\n",
    "    #ax = fig.add_subplot(2,1,1)\n",
    "    for n in range(0,16000+1,1000): \n",
    "\n",
    "        ax[0].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y')\n",
    "        ax[0].plot(time[0:n],Firing_target_AUDIO_MISS.T[0:n],'r',alpha=0.5)\n",
    "\n",
    "        ax[0].hlines(Threshold,-100,1500,color='grey',linestyle='dashed')\n",
    "        ax[0].vlines(RT_AUDIO[-1],0,Threshold,color='k',linestyle='dashed')\n",
    "        ax[0].set_xlabel('time(ms)')\n",
    "        ax[0].set_ylabel('Firing Rate')\n",
    "\n",
    "        ax[0].set_title(\"Auditory Trial\")\n",
    "        #plt.legend()\n",
    "        ax[1].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b')\n",
    "        ax[1].plot(time[0:n],Firing_target_VIS_MISS.T[0:n],'r',alpha=0.5)\n",
    "        ax[1].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "        ax[1].vlines(RT_VIS[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "        #plt.legend()\n",
    "        ax[1].set_xlabel('time(ms)')\n",
    "        ax[1].set_title(\"Visual Trial\")\n",
    "        ax[2].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b',label='Visual')\n",
    "        ax[2].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y',label='Audio')\n",
    "        #plt.plot(time,Firing_target_VIS_MISS.T)\n",
    "        ax[2].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "        ax[2].vlines(RT_AV[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "        if n<1000:\n",
    "            plt.legend(loc='lower right')\n",
    "        ax[2].set_xlabel('time(ms)')\n",
    "        ax[2].set_title(\"Audio-Visual Trial\")\n",
    "        plt.tight_layout()\n",
    "        camera.snap()\n",
    "        \n",
    "      #  ax.set_yticks([])\n",
    "     \n",
    "    animation = camera.animate()\n",
    "    animation.save('PRIMED_AV_xkcd.gif', writer='Pillow', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax= plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True,figsize=(14, 4), dpi=300)\n",
    "camera=Camera(fig)\n",
    "plt.style.use('bmh')\n",
    "\n",
    "#ax = fig.add_subplot(2,1,1)\n",
    "for n in range(0,16000+1,1000): \n",
    "\n",
    "    ax[0].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y')\n",
    "    ax[0].plot(time[0:n],Firing_target_AUDIO_MISS.T[0:n],'r',alpha=0.5)\n",
    "\n",
    "    ax[0].hlines(Threshold,-100,1500,color='grey',linestyle='dashed')\n",
    "    ax[0].vlines(RT_AUDIO[-1],0,Threshold,color='k',linestyle='dashed')\n",
    "    ax[0].set_xlabel('time(ms)')\n",
    "    ax[0].set_ylabel('Firing Rate')\n",
    "\n",
    "    ax[0].set_title(\"Auditory Trial\")\n",
    "    #plt.legend()\n",
    "    ax[1].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b')\n",
    "    ax[1].plot(time[0:n],Firing_target_VIS_MISS.T[0:n],'r',alpha=0.5)\n",
    "    ax[1].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "    ax[1].vlines(RT_VIS[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "    #plt.legend()\n",
    "    ax[1].set_xlabel('time(ms)')\n",
    "    ax[1].set_title(\"Visual Trial\")\n",
    "    ax[2].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b',label='Visual')\n",
    "    ax[2].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y',label='Audio')\n",
    "    #plt.plot(time,Firing_target_VIS_MISS.T)\n",
    "    ax[2].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "    ax[2].vlines(RT_AV[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "    if n<1000:\n",
    "        plt.legend(loc='lower right')\n",
    "    ax[2].set_xlabel('time(ms)')\n",
    "    ax[2].set_title(\"Audio-Visual Trial\")\n",
    "    plt.tight_layout()\n",
    "    camera.snap()\n",
    "\n",
    "  #  ax.set_yticks([])\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('AV_bmh.gif', writer='Pillow', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax= plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True,figsize=(14, 4), dpi=300)\n",
    "camera=Camera(fig)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#ax = fig.add_subplot(2,1,1)\n",
    "for n in range(0,16000+1,1000): \n",
    "\n",
    "    ax[0].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y')\n",
    "    ax[0].plot(time[0:n],Firing_target_AUDIO_MISS.T[0:n],'r',alpha=0.5)\n",
    "\n",
    "    ax[0].hlines(Threshold,-100,1500,color='grey',linestyle='dashed')\n",
    "    ax[0].vlines(RT_AUDIO[-1],0,Threshold,color='k',linestyle='dashed')\n",
    "    ax[0].set_xlabel('time(ms)')\n",
    "    ax[0].set_ylabel('Firing Rate')\n",
    "\n",
    "    ax[0].set_title(\"Auditory Trial\")\n",
    "    #plt.legend()\n",
    "    ax[1].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b')\n",
    "    ax[1].plot(time[0:n],Firing_target_VIS_MISS.T[0:n],'r',alpha=0.5)\n",
    "    ax[1].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "    ax[1].vlines(RT_VIS[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "    #plt.legend()\n",
    "    ax[1].set_xlabel('time(ms)')\n",
    "    ax[1].set_title(\"Visual Trial\")\n",
    "    ax[2].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b',label='Visual')\n",
    "    ax[2].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y',label='Audio')\n",
    "    #plt.plot(time,Firing_target_VIS_MISS.T)\n",
    "    ax[2].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "    ax[2].vlines(RT_AV[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "    if n<1000:\n",
    "        plt.legend(loc='lower right')\n",
    "    ax[2].set_xlabel('time(ms)')\n",
    "    ax[2].set_title(\"Audio-Visual Trial\")\n",
    "    plt.tight_layout()\n",
    "    camera.snap()\n",
    "\n",
    "  #  ax.set_yticks([])\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('AV_fivethirtyeight.gif', writer='Pillow', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax= plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True,figsize=(14, 4), dpi=300)\n",
    "camera=Camera(fig)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#ax = fig.add_subplot(2,1,1)\n",
    "for n in range(0,16000+1,1000): \n",
    "\n",
    "    ax[0].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y')\n",
    "    ax[0].plot(time[0:n],Firing_target_AUDIO_MISS.T[0:n],'r',alpha=0.5)\n",
    "\n",
    "    ax[0].hlines(Threshold,-100,1500,color='grey',linestyle='dashed')\n",
    "    ax[0].vlines(RT_AUDIO[-1],0,Threshold,color='k',linestyle='dashed')\n",
    "    ax[0].set_xlabel('time(ms)')\n",
    "    ax[0].set_ylabel('Firing Rate')\n",
    "\n",
    "    ax[0].set_title(\"Auditory Trial\")\n",
    "    #plt.legend()\n",
    "    ax[1].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b')\n",
    "    ax[1].plot(time[0:n],Firing_target_VIS_MISS.T[0:n],'r',alpha=0.5)\n",
    "    ax[1].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "    ax[1].vlines(RT_VIS[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "    #plt.legend()\n",
    "    ax[1].set_xlabel('time(ms)')\n",
    "    ax[1].set_title(\"Visual Trial\")\n",
    "    ax[2].plot(time[0:n],Firing_target_VIS_HIT.T[0:n],'b',label='Visual')\n",
    "    ax[2].plot(time[0:n],Firing_target_AUDIO_HIT.T[0:n],'y',label='Audio')\n",
    "    #plt.plot(time,Firing_target_VIS_MISS.T)\n",
    "    ax[2].hlines(Threshold,-100,1500,color='grey',linestyle='dashed',label='Threshold')\n",
    "    ax[2].vlines(RT_AV[-1],0,Threshold,color='k',linestyle='dashed',label='Reaction Time')\n",
    "    if n<1000:\n",
    "        plt.legend(loc='lower right')\n",
    "    ax[2].set_xlabel('time(ms)')\n",
    "    ax[2].set_title(\"Audio-Visual Trial\")\n",
    "    plt.tight_layout()\n",
    "    camera.snap()\n",
    "\n",
    "  #  ax.set_yticks([])\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('AV_ggplot.gif', writer='Pillow', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minV, maxV= 0.085,0.12\n",
    "# create one-dimensional arrays for x and y\n",
    "x = np.arange(minV, maxV+0.001, (maxV-minV)/7.)\n",
    "y = np.arange(minV, maxV+0.001, (maxV-minV)/7.)\n",
    "# create the mesh based on these arrays\n",
    "X, Y = np.meshgrid(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
